---
title: "ML extensions to OLS"
subtitle: "Oxford Spring School in Advanced Research Methods, 2021"
author: "Dr Thomas Robinson, Durham University"
date: "Day 2/5"
output: beamer_presentation
header-includes:
  - \usepackage{wrapfig}
  - \usepackage{graphicx}
  - \usepackage{bm}
  - \usepackage{soul}
  - \usepackage{hyperref}
  - \hypersetup{
    colorlinks=true,
    linkcolor=magenta,
    filecolor=magenta,      
    urlcolor=cyan,
    pdfpagemode=FullScreen,
    }
  - \usefonttheme[onlymath]{serif}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
```
## Introduction

Yesterday we explored how a familiar estimator (logistic regression) incorporates some fundamental aspects of ML

* ML is not some entirely new, alien type of doing statistics
* ML is typically focused on prediction problems
* Lots of really useful ML models are extensions of regression framework

So how should we understand OLS within a prediction context?

How do other popular forms of ML come out of it?

## Today's session

1. Recap OLS from prediction perspective

    * How does OLS work?
    * Optimisation criteria
    * Bias-variance trade-off

2. LASSO estimator

3. Hyperparameter tuning

4. Practical application

Key topics:

* Bias and variance
* Regularisation
* $K-$fold cross validation

# Ordinary Least Squares Regression

## Refresher: which is the best line?

```{r lobf, warning=FALSE, message=FALSE}

set.seed(89)

plot_data <- data.frame(x = runif(50,0,5))
plot_data$y <- 2 + 1.5*plot_data$x + rnorm(nrow(plot_data),0,1.5)

ggplot(plot_data, aes(x = x, y = y)) +
  geom_point(color = "dodgerblue2", size = 3) +
  stat_function(fun = function(x) 0.5*x+2, linetype = "solid", size = 1.2) + 
  stat_function(fun = function(x) 9-x, linetype = "dotted", size = 1.2) +
  stat_function(fun = function(x) 2+1.5*x, linetype = "dashed", size = 1.2) +
  theme_minimal() +
  
  geom_text(x = 1, y =9, label = "(a)", size=8) +
  geom_text(x = 3.25, y =7.5, label = "(b)", size=8) +
  geom_text(x = 2.5, y =2.75, label = "(c)", size=8) +
  theme(text = element_text(size = 20))

```

## Refresher: which is the best line? 2

```{r lobf2, warning=FALSE, message=FALSE}

plot_data <- data.frame(x = runif(50,0,5))
plot_data$y <- 2 + 1.5*plot_data$x + rnorm(nrow(plot_data),0,1.5)
plot_data$y_lin <- 2+1.5*plot_data$x
plot_data$y_curvy <- lm(y ~ poly(x, 14), data = plot_data)$fitted.values
  
curvy_plot <- ggplot(plot_data, aes(x = x, y = y)) +
  geom_point(color = "dodgerblue2", size = 3) +
  # stat_function(fun = function(x) 0.5*x+2, linetype = "solid", size = 1.2) + 
  # stat_function(fun = function(x) 9-x, linetype = "dashed", size = 1.2) +
  stat_function(fun = function(x) 2+1.5*x, size = 1.2, color = "black", linetype = "dashed") +
  geom_smooth(method = lm, formula = y ~ poly(x, 14), se=FALSE, color = "red") +
  ylim(0,12) +
  theme_minimal() +
  theme(text = element_text(size = 20))

curvy_plot
```

## Refresher: which is the best line? 2

```{r lobf2_mae, warning=FALSE, message=FALSE}

mae_lin <- mean(abs(plot_data$y - plot_data$y_lin))
mae_curvy <- mean(abs(plot_data$y - plot_data$y_curvy))

curvy_plot +
  geom_text(x = 4, y = 2.75, size = 8, 
            label = paste("MAE:",round(mae_lin,3))) +
  geom_text(x = 4, y = 1.25, size = 8, 
            label = paste("MAE:",round(mae_curvy,3 )),
            color = "red")

```


## OLS as a tool for inference

In inference terms, OLS estimates $\bm{\hat{\beta}}$ that:

* Captures the linear relationship between $\mathbf{X}$ and $\mathbf{y}$
* Yields individual estimates of the "effects" of $\bm{X}$ on $\bm{y}$
* Allows us to understand the uncertainty over $\bm{\hat{\beta}}$

  * E.g., how confident are we that there is +/- effect of $\bm{x_1}$ on $\bm{y}$
  
## OLS: Optimisation

OLS regression minimises the **sum of the squared error** between the regression line ($\bm{X\beta}$) and the observed outcome ($\mathbf{y}$):

$$
\operatorname*{arg\,min}_{\bm{\beta}} \sum_{i=1}^N(y_i-\bm{x_i\beta})^2,
$$

where $\bm{x_i\beta}$ is the linear regression function.

*How might we solve this?*

* Calculus -- there is a closed form solution (unlike logistic regression)
* Maximum likelihood estimation

## Why do we like OLS?

Not only does OLS have a closed form solution, we also know that, under the Gauss Markov (GM) assumptions, OLS is:

* \textbf{\textcolor{blue}{B}}est
* \textcolor{blue}{\st{L}}\st{inear} \href{https://www.ssc.wisc.edu/~bhansen/papers/gauss.pdf}{(Hansen 2021)}
* \textbf{\textcolor{blue}{U}}nbiased
* \textbf{\textcolor{blue}{E}}stimator

\hyperlink{gm_ass}{\beamerbutton{GM Assumptions}}

In other words, in terms of estimating the parameters $\bm{\beta}$, you won't find a linear model that is unbiased with a lower variance

OLS is fantastic for inference:

* Typically concerned with generating **unbiased** estimate of $\bm{\hat{\beta}}$ 
* So we can perform valid significance testing

## Bias

Bias is a feature of the estimator:

* $\text{Bias}_{\bm{\beta}} = \big(\mathbb{E}[\bm{\hat{\beta}}] - \bm{\beta}\big)$

* On average, the estimated parameters are equal to the true parameters

* Under GM, we know that $\big(\mathbb{E}[\bm{\hat{\beta}}] - \bm{\beta}\big) = 0$


## Variance

As we sample new (GM-satisfying) data, the parameters of our model will shift:

* Hence, there will be variance over our parameter estimates

* $\mathbb{V}_{\bm{\hat{\beta}}} = \mathbb{E}\big[(\mathbb{E}[\bm{\hat{\beta}}] - \bm{\hat{\beta}})^2\big]$

* The average distance between a particular parameter estimate and the mean of parameter estimates over multiple samples

## Visualising bias and variance

```{r bias-var,warning=FALSE, message=FALSE,fig.align='center', fig.width = 3.5, fig.height=3.5}

circleFun <- function(center = c(0,0),diameter = 1, npoints = 100){
    r = diameter / 2
    tt <- seq(0,2*pi,length.out = npoints)
    xx <- center[1] + r * cos(tt)
    yy <- center[2] + r * sin(tt)
    return(data.frame(x = xx, y = yy))
}

c1 <- circleFun(c(0,0)) %>% mutate(bias = "High bias", variance = "High variance")
c2 <- circleFun(c(0,0)) %>% mutate(bias = "Low bias", variance = "High variance")
c3 <- circleFun(c(0,0)) %>% mutate(bias = "High bias", variance = "Low variance")
c4 <- circleFun(c(0,0)) %>% mutate(bias = "Low bias", variance = "Low variance")

set.seed(89)
hbhv <- data.frame(x = rnorm(10,0.2,0.14),
                   y = rnorm(10,0.2,0.14),
                   bias = "High bias", variance = "High variance")

hblv <- data.frame(x = rnorm(10,0.2,0.03),
                   y = rnorm(10,0.2,0.03),
                   bias = "High bias", variance = "Low variance")

lbhv <- data.frame(x = rnorm(10,0,0.14),
                   y = rnorm(10,0,0.14),
                   bias = "Low bias", variance = "High variance")

lblv <- data.frame(x = rnorm(10,0,0.03),
                   y = rnorm(10,0,0.03),
                   bias = "Low bias", variance = "Low variance")

points_df <- rbind(hbhv, hblv, lbhv, lblv)

ggplot(rbind(c1,c2,c3,c4),aes(x,y)) + 
  geom_path() + 
  geom_point(data = points_df, aes(x = x, y = y), size = 2, color = "red", alpha = 0.7) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  facet_grid(bias ~ variance) +
  xlim(-0.5,0.5) + ylim(-0.5,0.5) +
  labs(x = "", y = "") +
  theme_minimal() +
  theme(text = element_text(size = 14),
        axis.ticks = element_blank(),
        axis.text = element_blank())
  
```

## Predicting *new* values

In the remainder of today's session, we are going to consider the following generic supervised learning problem:

* We observe $(\bm{y,X}) \in \mathcal{D}$
  * A training sample that is taken from a wider possible set of data
  * I.e. we can think, counterfactually, of resampling to get a new sample $(\bm{y_\text{New},X_\text{New}})$

* We also observe a "test" dataset $\bm{X'}$

The goal is to estimate $\bm{y'}$ by training a model $\hat{f}$

  * The outcomes that correspond to $\bm{X'}$



## OLS as a tool for prediction

When we run OLS, we also get a "trained model":

* $\hat{f}$ -- that has parameters equal to $\bm{\hat{\beta}}$
* Can be applied to a new "test" dataset $\bm{X'}$
* To generate new predictions $\bm{y'}$

## Bias and variance of predictions

We can also think of bias in terms of the predictions:

* $\text{Bias}_{\bm{y}} = \big(\mathbb{E}[\bm{\hat{y}}] - \bm{y}\big)$

* We ideally want low bias

* High bias suggests the model is not sensitive enough

And we can think about the variance of the prediction:

* $\mathbb{V}_{\bm{\hat{y}}} = \mathbb{E}\big[(\mathbb{E}[\bm{\hat{y}}] - \bm{\hat{y}})^2\big]$

High variance means that the model is very sensitive to $\bm{X}$ -- the training data -- but will perform poorly on new samples of data

* With the new data, and high variance, we would expect quite different predictions

## Bias-variance trade off

So can't we just choose a low-variance, low-bias modeling strategy? Not quite!

Assume we calculate the mean squared error of some new data $\bm{X'}$ given a trained model $\hat{f}$:
$$
\text{MSE} = \mathbb{E}[(\hat{f}(\bm{X'})  - \bm{y})^2].
$$
We can decompose this further:
$$
MSE = \underbrace{\mathbb{E}\big[(\hat{f}(\bm{X'})-\mathbb{E}[\bm{\hat{y}}])^2\big]}_{\text{Variance}} + \underbrace{\big(\mathbb{E}[\bm{\hat{y}}] - \bm{y}\big)^2}_{\text{Bias}^2}
$$

So holding the MSE fixed, if we reduce the variance we must increase the bias

* I.e. there is a **bias-variance trade-off**

## Visualising the trade-off

```{r tradeoff, warning=FALSE, message=FALSE}

err_df <- data.frame(complexity = seq(0.5,4,by = 0.2))
err_df$var <- 0.1*err_df$complexity^2
err_df$bias2 <- 1/(err_df$complexity)
err_df$total <- err_df$var + err_df$bias2 + 0.1

err_labs <- list("Variance",bquote("Bias"^2),"Total")

err_df %>% 
  pivot_longer(-complexity) %>% 

ggplot(aes(x = complexity, y = value, color = name)) +
  geom_line(size = 1.2) +
  geom_point(size = 3) +
  geom_vline(xintercept = 1.7, linetype = "dashed", size = 1.2) + 
  scale_colour_manual(values=1:3,breaks=c("var","bias2","total"),
                      labels=err_labs) + 
  labs(x = "Model Complexity", y = "Error",color = "Error") + 
  theme_minimal() +
    theme(legend.position = "bottom",
          axis.text = element_blank(),
          axis.line = element_line(arrow = grid::arrow(length = unit(0.3, "cm"), 
                                                       ends = "last"),
                                   size = 1.2),
          axis.title = element_text(hjust = 0.9),
          text = element_text(size = 20))

```

## Out of sample performance of OLS

This trade-off explains why we might not want to use OLS for prediction tasks:

* By virtue of GM assumptions and BLUE, MSE is explained entirely by variance

    * Averaging across models, the model parameters are centred on the true values (an unbiased estimator)
    
* So we cannot tweak the model to get slightly better out-of-sample predictions at the expense of some added bias

  * In other words, we cannot leverage the bias-variance trade-off
  
## OLS: Complex OLS model trained on $\mathbf{X}$

```{r complex_mod, warning=FALSE, message=FALSE}

ggplot(plot_data, aes(x = x, y = y)) +
  geom_point(color = "dodgerblue2", size = 3) +
  geom_smooth(method = lm, formula = y ~ poly(x, 14), se=FALSE, color = "red") + 
  ylim(0,12) +
  theme_minimal() +
  theme(text = element_text(size = 20))

```
  
## OLS: Compare models' predictions to $\mathbf{X'}$

```{r lobf3, warning=FALSE, message=FALSE}

set.seed(1037)
new_data <- data.frame(x = runif(50,0,5))
new_data$y <- 2 + 1.5*new_data$x + rnorm(nrow(new_data),0,1.5)

ggplot(plot_data, aes(x = x, y = y)) +
  # geom_point(color = "dodgerblue2", size = 3) +
  geom_point(data = new_data, aes(x = x, y = y), color = "orchid3", size = 3) +
  # stat_function(fun = function(x) 0.5*x+2, linetype = "solid", size = 1.2) + 
  # stat_function(fun = function(x) 9-x, linetype = "dashed", size = 1.2) +
  # stat_function(fun = function(x) 2+1.5*x, size = 1.2, color = "black", linetype = "dashed") +
  geom_smooth(method = lm, formula = y ~ poly(x, 14), se=FALSE, color = "red") + 
  ylim(0,12) +
  theme_minimal() +
  theme(text = element_text(size = 20))

```

## OLS: Variance in model predictions over $\mathbf{X}$

```{r lobf4, warning=FALSE, message=FALSE}

ggplot(new_data, aes(x = x, y = y)) +
  # geom_point(color = "dodgerblue2", size = 3) +
  geom_point(color = "orchid3", size = 3) +
  # stat_function(fun = function(x) 0.5*x+2, linetype = "solid", size = 1.2) + 
  # stat_function(fun = function(x) 9-x, linetype = "dashed", size = 1.2) +
  # stat_function(fun = function(x) 2+1.5*x, size = 1.2, linetype = "dashed") +
  geom_smooth(data = plot_data, aes(x = x, y=y), method = lm, formula = y ~ poly(x, 14), se=FALSE, color = "red") + 
  geom_smooth(method = lm, formula = y ~ poly(x, 14), se=FALSE, color = "orchid3") + 
  ylim(0,12) +
  theme_minimal() +
  theme(text = element_text(size = 20))

```

# The LASSO estimator

## Regularisation and overfitting

In the previous examples, an overly complicated model yields poor out-of-sample predictions. To ensure our model does not have too much variance, we can *penalise* overly-complicated models that will perform poorly on new test data ($\bm{X'}$)

 * This may introduce bias into the model
 
 * But, if done correctly, we can reduce the total MSE by offsetting overly-high variance
 
 * And therefore yield better predictions on $\bm{X'}$

This is a generalisable feature of ML:

  * Regularisation constrains model complexity to prevent overfitting

  * Especially important for the models we consider in the remainder of the week that are very powerful
  
## Regularisation of OLS

If we want to continue using a linear predictor we need to modify the loss function $L$:

* $L_\text{OLS}$ is known to be unbiased

* So adding a term that is non-zero to $L$ will *add* bias...

* ... and hopefully improve out-of-sample prediction

In other words:

* We sacrifice some variance in order to improve the predictive performance of the model on $\bm{X'}$

## Generalising OLS with regularization

We can state this problem using the following general linear optimisation problem:

$$
\operatorname*{arg\,min}_f \underbrace{\sum_{i=1}^N(y_i-f(\bm{x_i}))^2}_\text{Sum of squared error} + \underbrace{\lambda R(f)}_\text{Regularisation}
$$
For OLS:

* $f \in \mathcal{F}_\text{linear}$
* $\lambda = \frac{1}{\infty} = 0$ 

But what if $\lambda \neq 0$?

* Then we must decide what $R(\cdot)$ is
* And decide on a value of $\lambda$ -- a hyperparameter

## $R(\cdot)$ as shrinkage

Consider an OLS model with $k$ parameters:

* The model estimates coefficients for each parameter
* Regardless of how large or small that coefficient is
* In a sense, with non-zero estimates for each parameter these models can be considered "complex"

We can reduce the complexity of the regression model by setting some parameters to zero

* I.e. we **shrink** the coefficient estimates

* Aim to reduce the variance error by more than the increase in bias error

In the linear framework, we need some way to penalize non-zero coefficients

## Least absolute shrinkage and selection operator (LASSO)

We can calculate the total magnitude (or **L1 Norm**) of the coefficients in a model as:
$$
||\bm{\beta}||_1 = \sum_j |\beta_j|
$$
Next, we can think about restricting the size of this norm:
$$
||\bm{\beta}||_1 \leq t
$$
And finally we want to include this in our loss function:
$$
\operatorname*{arg\,min}_{\bm{\beta}} \sum_{i=1}^N(y_i-\bm{x_i\beta})^2 \text{ subject to } ||\bm{\beta}||_1 \leq t
$$
This final optimisation constraint is equivalent to:
$$
\operatorname*{arg\,min}_{\bm{\beta}} \sum_{i=1}^N(y_i-\bm{x_i\beta})^2 + \lambda||\bm{\beta}||_1.
$$

## LASSO 

Hence, the LASSO estimator conforms to the generalised loss function introduced earlier

* $R(f) = ||\bm{\hat{\beta}}||_1$

LASSO often yields coefficient estimates of exactly zero:

* Think about varying the true value of some coefficient $\beta_j$:

  * When $\beta_j$ is large, we might shrink it (relative to OLS) but the importance of this predictor is sufficient to entail a non-zero coefficient

  * But for some small enough value $b$, the cost of including $b$ in L1-norm is greater than the reduction in squared error
  
* In other words, the L1 norm constraint can lead to "corner solutions"
  
  
## Example of LASSO corner solution ($||\bm{\beta}||_1 \leq 1$)

```{r zero_coef, warning=FALSE, message=FALSE}

library(MASS)

set.seed(89)

ellipse_mkr <- function(mu, sigma) {
  
  n = 100
  Sigma = matrix(c(sigma[1], sigma[2],sigma[2],sigma[1]), nrow=2)
  theta = 180*pi/180 #Angle in radians
  mu2 = c(mu[1]*cos(theta) - mu[2]*sin(theta),mu[2]*cos(theta) + mu[1]*sin(theta))
  rot <-matrix(c(cos(theta),sin(theta),-sin(theta),cos(theta)),nrow=2)
  ols_df <- as.data.frame(mvrnorm(n=n, mu=mu, Sigma=rot %*% Sigma %*% t(rot)))
  
  return(ols_df)
}


constraint_df <- data.frame(beta1 = c(0,1,0,-1),
                            beta2 = c(1,0,-1,0))

ols_df_int <- ellipse_mkr(mu = c(1,3), sigma = 1.15*c(.75,.5))
ols_df_small <- ellipse_mkr(mu = c(1,3), sigma = 0.5*c(.75,.5))
ols_df_smallest <- ellipse_mkr(mu = c(1,3), sigma = 0.2*c(.75,.5))

ggplot(constraint_df, aes(x=beta1, y=beta2)) +
  
  geom_text(x = 1, y = 3, label = expression(hat(beta)), fontface = "bold", size = 6) +
  geom_hline(yintercept = 0,) +
  geom_vline(xintercept = 0) + 
  geom_polygon(fill = "firebrick2") +
  stat_ellipse(data = ols_df_int, aes(x = V1, y = V2),
               size = 1.2) +
  stat_ellipse(data = ols_df_small, aes(x = V1, y = V2),
               size = 1.2) +
  stat_ellipse(data = ols_df_smallest, aes(x = V1, y = V2),
               size = 1.2) + 
  labs(x = expression(beta[1]), y = expression(beta[2])) + 
  theme_minimal() + 
  theme(text = element_text(size = 20))
  
```

## Comparison of $\hat{\beta}_j^\text{OLS}$ to $\hat{\beta}_j^\text{LASSO}$

```{r lasso-ols, warning=FALSE, message=FALSE}

ggplot(data.frame(x = 0)) +
  geom_abline(linetype = "dashed", size = 1.2) +
  stat_function(fun = function(x) {ifelse(x < 1, 0, (x-1))},
                n = 10000,
                size = 1.2, color = "dodgerblue2") + 
  labs(x = expression(hat(beta)[j]^"OLS"),
       y = expression(hat(beta)[j]^"LASSO")) +
  xlim(0,3) + 
  ylim(0,3) + 
  theme_minimal() + 
  theme(text = element_text(size = 20))

```


## Two helpful properties of LASSO

1. Prediction accuracy

    * We trade off an amount of bias for a (hopefully) greater reduction in variance, improving out-of-sample prediction
    * Cf. a non-zero *true* coefficient estimated with a large confidence interval

2. Selection of relevant variables

    * The possibility of corner-solutions acts as a useful variable selection mechanism
    
    * LASSO essentially selects the most important variables for us
    
# Hyperparameter tuning

## Choosing $\lambda$

The final part of the estimation problem is setting $\lambda$. Recall that:
$$
\mathcal{L} = \sum_{i=1}^N\big(y_i-f(x_i)\big)^2 + \lambda||\bm{\hat{\beta}}||_1
$$

$\lambda$ regulates how much bias we add to the model:

* Too large a value = overly constricted model, large MSE
* Too small a value = overly complex model, large MSE

We need to find a value that helps us get near the bottom of the total-error curve!

* This process is called hyperparameter tuning
* It is a recurrent feature of ML methods

## Simple tuning

Simplest way is to simply try a few values:

* In the case of LASSO, we might try $\lambda = \{0.1,1,10\}$
* Choose $\lambda$ that yields lowest MSE from $MSE_{\lambda=0.1},MSE_{\lambda=1},MSE_{\lambda=10}$
* Use this value in the final model

But there are some limitations:

* You are "testing" your model on the same data that it was trained upon
* So this will inflate the actual accuracy of your model
* Goes against the train-test ethos of ML prediction

## Holdout sample

As an alternative, we could create a holdout sample:

* Split our training data $\bm{X}$ into $\{\bm{X^\text{Train},X^\text{Holdout}}\}$
* Train our model for each value of $\lambda$ on $\bm{X^\text{Train}}$
* Then test the predictive accuracy on $\bm{X^\text{Holdout}}$
* In other words, create a miniature version of the train-test split within our training data

But again, there are limitations:

* By leaving out some observations, we lose predictive power
* Even if observations are randomised across the two datasets, the model can never learn from the fixed holdout data

## $K$-fold cross validation

We can generalise holdout sampling to incorporate all of our training data:

1. Randomly assign each *training* observation to one of $k$ **folds**
2. Estimate the model $k$ times, omitting one "fold" from training at a time
3. Calculate the prediction error using the fold not included in the data
4. Average the prediction errors across $k-folds$
5. Repeat 2-4 for each value of $\lambda$ we want to test

Choose $\lambda$ where the average cross-validated MSE is lowest

The choice of $k$ will depend on:

* The time it takes to train the model
* The size of your training data

# Application
 
## Blackwell and Olsen (2021)

Suppose we have an outcome $\bm{y}$, a treatment $\bm{d}$, covariates $\bm{X}$, and an "effect moderator" $\bm{v}$

* We want to estimate an *inference* model
* Understand how the treatment effect is moderated

Naive suggestion:

* Include an interaction term to model the differential effect of treatment
  * I.e. $y_i = \beta_0 + \beta_1d_i + \beta_2v_i + \beta_3d_iv_i + \bm{\beta'X_i}$

What's wrong with this model?

* We assume that the interactive effect $\beta_3$ is constant across covariates
* This introduces bias into the model if $\bm{vX}$ is related either to $\bm{dv}$ or $\bm{y}$

## Prediction and inference problem

Therefore the researcher faces a prediction problem *and* an inference problem:

* **Inference problem**: How do we control for potential bias introduced between $\bm{X,v,d}$, and $\bm{y}$?


* **Prediction problem**: Which interactions within $\bm{vX}$ are most likely to confound the results?

  * Let us denote the true non-zero predictors $\mathcal{P}$
  
  * Inverting a $\bm{\hat{y}}$ problem -- which variables are useful to predict new data?

From today's session we know that:

* Bias can be useful to offset variance when making out-of-sample predictions 

* Bias inherently distorts our estimate of $\bm{\beta}$


## Combining LASSO and OLS

Blackwell and Olsen propose splitting the problem of interaction estimation into two stages:

1. **Variable selection**
    
    * Use LASSO to estimate a series of variable selection models
    * Attempt to find interaction terms that correlate with either outcome, treatment, or treatment-moderated interaction
      
2. **Inference**

    * Use OLS to estimate an inference model
    * Using only non-zero interaction terms in LASSO models
    
What makes this strategy so useful (and informative!) is that:

* We leverage bias to make better predictions in Stage 1
* We de-bias inference in Stage 2 using OLS + Stage 1 results

## Post-double selection method

**Stage 1**

* Estimate LASSO models for:
  1. $\bm{y}$ on $\{\bm{v,X,vX}\}$ 
  2. $\bm{d}$ on $\{\bm{v,X,vX}\}$
  3. $\bm{dv}$ on $\{\bm{v,X,vX}\}$

* Let $\bm{Z*}$ index all variables with non-zero coefficients in any of models 1-3

**Stage 2**

* Regress $\bm{y}$ on $\bm{d, dv}$ and $\bm{Z*}$

Blackwell and Olson also suggest adding all "base-terms" (i.e $\bm{X}$) regardless of LASSO coefficient

# Extra Slides

## Alternative $R(f)$ to the L1 norm

Following a similar logic to the shrinkage used by LASSO, we can define other measures of magnitude, like the L2 norm $||\bm{\beta}||_2$:
$$
\sqrt{\sum_j{|\beta_j|^2}}
$$

When we plug in the L2 norm into the loss function, we get the **ridge regression** estimator:
$$
\operatorname*{arg\,min}_{\bm{\beta}} \sum_{i=1}^N(y_i-\bm{x_i\beta})^2 + \lambda||\bm{\beta}||_2
$$
Unlike the LASSO estimator, ridge regression does not have a sharp cut-off, but rather scales the size of all coefficients in the model

## Ridge regression -- no corner solutions

```{r zero_coef_ridge, warning=FALSE, message=FALSE}

library(MASS)

set.seed(89)

constraint_df <- circleFun(c(0,0), diameter = 2)

ols_df_int <- ellipse_mkr(mu = c(1,3), sigma = 1.15*c(.75,.5))
ols_df_small <- ellipse_mkr(mu = c(1,3), sigma = 0.5*c(.75,.5))
ols_df_smallest <- ellipse_mkr(mu = c(1,3), sigma = 0.2*c(.75,.5))

ggplot(constraint_df, aes(x=x, y=y)) +
  
  geom_text(x = 1, y = 3, label = expression(hat(beta)), fontface = "bold", size = 6) +
  geom_hline(yintercept = 0,) +
  geom_vline(xintercept = 0) + 
  geom_polygon(fill = "firebrick2") + 
  stat_ellipse(data = ols_df_int, aes(x = V1, y = V2),
               size = 1.2) +
  stat_ellipse(data = ols_df_small, aes(x = V1, y = V2),
               size = 1.2) +
  stat_ellipse(data = ols_df_smallest, aes(x = V1, y = V2),
               size = 1.2) + 
  labs(x = expression(beta[1]), y = expression(beta[2])) + 
  theme_minimal() + 
  theme(text = element_text(size = 20)) + 
  xlim(-1.5,3.5) + ylim(-1.5,3.5)
  
```

## Ridge regression -- constant scaling of coefficients

```{r ridge-ols, warning=FALSE, message=FALSE}

ggplot(data.frame(x = 0)) +
  geom_abline(linetype = "dashed", size = 1.2) +
  stat_function(fun = function(x) 0.5*x,
                n = 10000,
                size = 1.2, color = "orchid2") + 
  labs(x = expression(hat(beta)[j]^"OLS"),
       y = expression(hat(beta)[j]^"Ridge")) +
  xlim(0,3) + 
  ylim(0,3) + 
  theme_minimal() + 
  theme(text = element_text(size = 20))

```


## Gauss Markov Assumptions
\label{gm_ass}

Five assumptions need to hold:

1. $\bm{y}$ is a linear function of $\bm{\beta}$

2. $\mathbb{E}[\epsilon_i] = 0$

3. $\mathbb{V}[\epsilon_i] = \sigma^2_i, \forall i$

4. $Cov(\epsilon_i,\epsilon_j) = 0, \forall i \neq j$

5. $Cov(\bm{x_i},\epsilon_i) = 0$
